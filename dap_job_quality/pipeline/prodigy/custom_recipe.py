"""
Custom prodigy recipe to use the NER model to extract BENEFITS and allow for
free text entry.

To run the custom recipe:

prodigy benefits_classification job_quality_annotated \
    dap_job_quality/pipeline/prodigy/labelled_data/20240116_ads_to_label.jsonl \
    -F dap_prinz_green_jobs/pipeline/prodigy/custom_recipe.py
"""
from dap_job_quality import PROJECT_DIR, logger

from typing import Iterator
from pathlib import Path

import prodigy
import spacy
import copy

from prodigy.components.loaders import JSONL
from prodigy.components.preprocess import add_tokens

# LOAD NER MODEL
model_folder = PROJECT_DIR / "outputs/models/ner_model/20230808"
if not model_folder.exists():
    logger.error(
        f"Model folder {model_folder} does not exist. Please download the model."
    )

nlp = spacy.load(model_folder)


def make_tasks(
    nlp: spacy.language.Language,
    stream: Iterator[dict],
) -> Iterator[dict]:
    """Add predicted entities generated by custom NER model and
        option for free text entry to Prodigy stream.

    Args:
        nlp (spacy.language.Language): spaCy language model

    Yields:
        Iterator[dict]: Iterator of dictionaries with text and spans keys
    """
    for eg in stream:
        doc = nlp(eg["text"])
        spans = []
        task = copy.deepcopy(eg)
        for ent in doc.ents:
            if ent.label_ == "BENEFIT":
                start = ent.start_char
                end = ent.end_char
                token_start = ent.start
                token_end = ent.end
                spans.append(
                    {
                        "start": start,
                        "end": end,
                        "token_start": token_start,
                        "token_end": token_end,
                        "label": ent.label_,
                    }
                )
        task["spans"] = spans

        # Add a free text entry option
        task["options"] = [{"id": 0, "text": "Free text entry"}]

        yield task


@prodigy.recipe(
    "benefits_ner_classification",
    dataset=("The dataset to use", "positional", None, str),
    source=("The source data as a .jsonl file", "positional", None, Path),
)
def custom_ner(dataset, source):
    # Initialize the Prodigy stream
    stream = JSONL(source)

    # Add tokens to the stream
    stream = add_tokens(nlp, stream)

    # Add predicted entities and text to the stream
    stream = make_tasks(nlp, stream)

    return {
        "dataset": dataset,  # save annotations in this dataset
        "view_id": "ner_manual",  # use the ner_manual interface
        "stream": stream,
        "config": {
            "buttons": ["accept", "reject", "ignore"],
            "labels": ["BENEFIT"],  # the label for the manual NER interface
        },
    }
