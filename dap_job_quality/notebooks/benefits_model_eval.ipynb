{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import srsly\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from dap_job_quality import PROJECT_DIR, BUCKET_NAME\n",
    "from dap_job_quality.getters.data_getters import load_s3_jsonl\n",
    "import dap_job_quality.utils.prodigy_data_utils as pdu\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other helper functions\n",
    "def tokenize(text, n=2):\n",
    "    \"\"\"Tokenize text into n-grams\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "def most_common_ngrams(df, n=2, label_col='label', text_col='labelled_span', n_most_common=10):\n",
    "    \"\"\"Find the most common n-grams within a category\n",
    "    \"\"\"\n",
    "    category_ngrams = {}\n",
    "    for category in df[label_col].unique():\n",
    "        ngrams_list = []\n",
    "        for text in df[df[label_col] == category][text_col]:\n",
    "            n_grams = tokenize(text, n)\n",
    "            ngrams_list.extend(n_grams)\n",
    "        category_ngrams[category] = Counter(ngrams_list).most_common(n_most_common)\n",
    "    return category_ngrams\n",
    "\n",
    "def find_phrase_and_sentence(text, phrases):\n",
    "    \"\"\"Find a phrase in a text and return the whole sentence containing the phrase\n",
    "    \"\"\"\n",
    "    for phrase in phrases:\n",
    "        if phrase in text.lower():  # Check if the phrase is in the text\n",
    "            # Find the whole sentence containing the phrase\n",
    "            sentence = re.search(r'([^.]*?'+re.escape(phrase)+r'[^.]*\\.)', text, re.IGNORECASE)\n",
    "            if sentence:\n",
    "                return True, sentence.group()\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file1 = PROJECT_DIR / 'dap_job_quality/pipeline/prodigy/labelled_data/benefits_model_eval.jsonl'\n",
    "\n",
    "_ = load_s3_jsonl(BUCKET_NAME, 'job_quality/prodigy/labelled_data/benefits_model_eval.jsonl', local_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "\n",
    "for file in [local_file1]:\n",
    "    records = []\n",
    "    for line in srsly.read_jsonl(file):\n",
    "        records.append(line)\n",
    "    for record in records:\n",
    "        all_records.append(record)\n",
    "        \n",
    "all_records_deduplicated = []\n",
    "seen_job_ids = set()\n",
    "\n",
    "for item in all_records:\n",
    "    job_id = item['meta']['job_id']\n",
    "    if job_id not in seen_job_ids:\n",
    "        seen_job_ids.add(job_id)\n",
    "        all_records_deduplicated.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = pd.DataFrame(all_records_deduplicated)\n",
    "all_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records[all_records['answer']!='accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_spans = pdu.get_spans_and_sentences(all_records[all_records['answer']!='accept'].to_dict(orient='records'))\n",
    "rejected_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_spans = pdu.get_spans_and_sentences(all_records_deduplicated)\n",
    "accepted_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "for job_id, entries in accepted_spans.items():\n",
    "    for entry in entries:\n",
    "        flat_data.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"labelled_span\": entry[\"span\"],\n",
    "            \"full_sentence\": entry[\"sent\"],\n",
    "            \"label\": entry[\"label\"],\n",
    "            \"text\": entry[\"text\"]\n",
    "        })\n",
    "\n",
    "labelled_spans_df = pd.DataFrame(flat_data)\n",
    "\n",
    "labelled_spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_spans_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelled_spans_df['job_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 31/(31+2)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = 31/(31+70)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure the wordclouds tell us very much because it was such a small sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for each label\n",
    "label_categories = labelled_spans_df['label'].unique()\n",
    "label_categories = label_categories[(label_categories != 'none')]\n",
    "\n",
    "# Create a subplot for each wordcloud in a 1x2 configuration\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, label in enumerate(label_categories):\n",
    "    if i < 2:  # Ensure we don't go out of bounds\n",
    "        ax = axes[i]\n",
    "        text = ' '.join(labelled_spans_df[labelled_spans_df['label'] == label]['labelled_span'].tolist())\n",
    "        wordcloud = WordCloud(width=800, height=800, \n",
    "                              background_color ='white',\n",
    "                              min_font_size = 10).generate(text)\n",
    "        \n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(label)\n",
    "\n",
    "# This will ensure that any extra subplots are not visible\n",
    "for j in range(i+1, 2):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(PROJECT_DIR / 'outputs/figures/wordclouds_benefits.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_job_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
