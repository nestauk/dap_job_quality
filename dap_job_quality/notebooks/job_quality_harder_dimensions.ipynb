{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harder dimensions of job quality: \"job design and nature of work\", \"social support and cohesion\", \"voice and representation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "import srsly\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from dap_job_quality import PROJECT_DIR\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def tokenize(text, n=2):\n",
    "    \"\"\"Tokenize text into n-grams\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "def most_common_ngrams(df, n=2, label_col='label', text_col='labelled_span', n_most_common=10):\n",
    "    \"\"\"Find the most common n-grams within a category\n",
    "    \"\"\"\n",
    "    category_ngrams = {}\n",
    "    for category in df[label_col].unique():\n",
    "        ngrams_list = []\n",
    "        for text in df[df[label_col] == category][text_col]:\n",
    "            n_grams = tokenize(text, n)\n",
    "            ngrams_list.extend(n_grams)\n",
    "        category_ngrams[category] = Counter(ngrams_list).most_common(n_most_common)\n",
    "    return category_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample of ~60 job ads labelled for \"voice and representation\", \"social support and cohesion\" and \"job design and nature of work\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = PROJECT_DIR / 'dap_job_quality/pipeline/prodigy/labelled_data/20240119_ads_labelled_rosie.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data as a dataframe first of all, to see what fields it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL data\n",
    "data = []\n",
    "for line in srsly.read_jsonl(file):\n",
    "    data.append(line)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in and find the labelled spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for line in srsly.read_jsonl(file):\n",
    "    if line[\"answer\"] == \"accept\":\n",
    "        records.append(line)\n",
    "\n",
    "training_data = {}\n",
    "\n",
    "for record in records:\n",
    "    # convert each text to a spacy document\n",
    "    doc = nlp(record['text'])\n",
    "    all_sents = list(doc.sents)\n",
    "    # get the labelled spans within each document\n",
    "    spans = record[\"spans\"]\n",
    "    spans_parsed = []\n",
    "    # map the span back to the text it corresponds to\n",
    "    for span in spans:\n",
    "        span_data = {}\n",
    "        span_data[\"sent\"] = Span(\n",
    "                        doc,\n",
    "                        span[\"token_start\"],\n",
    "                        span[\"token_end\"] + 1,\n",
    "                        span[\"label\"],\n",
    "                    ).text\n",
    "        span_data[\"label\"] = span[\"label\"]\n",
    "        span_data[\"text\"] = record['text']\n",
    "        spans_parsed.append(span_data)\n",
    "    training_data[record[\"meta\"][\"job_id\"]] = spans_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing the parsed, labelled spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "for job_id, entries in training_data.items():\n",
    "    for entry in entries:\n",
    "        flat_data.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"labelled_span\": entry[\"sent\"],\n",
    "            \"label\": entry[\"label\"],\n",
    "            \"text\": entry[\"text\"]\n",
    "        })\n",
    "\n",
    "labelled_spans_df = pd.DataFrame(flat_data)\n",
    "\n",
    "labelled_spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_spans_df.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of labels in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for the distribution of labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "labelled_spans_df['label'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the sentences that occur under each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What comes under `other_benefits`?\n",
    "other_benefits = labelled_spans_df[labelled_spans_df['label'] == 'other_benefits']\n",
    "other_benefits[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about `6_voice_representation`?\n",
    "voice_representation = labelled_spans_df[labelled_spans_df['label'] == '6_voice_representation']\n",
    "voice_representation[['labelled_span']]\n",
    "# could have just regexed \"equal opportunities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_support = labelled_spans_df[labelled_spans_df['label'] == '5_social_support_cohesion']\n",
    "social_support[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for each label\n",
    "label_categories = labelled_spans_df['label'].unique()\n",
    "label_categories = label_categories[(label_categories != 'benefit') & (label_categories != 'other_benefits')]\n",
    "\n",
    "# Create a subplot for each wordcloud\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))\n",
    "fig.suptitle('Wordclouds for Each Label', fontsize=16)\n",
    "\n",
    "for i, label in enumerate(label_categories):\n",
    "    ax = axes[i]\n",
    "    text = ' '.join(labelled_spans_df[labelled_spans_df['label'] == label]['labelled_span'].tolist())\n",
    "    wordcloud = WordCloud(width=800, height=800, \n",
    "                          background_color ='white',\n",
    "                          min_font_size = 10).generate(text)\n",
    "    \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = most_common_ngrams(labelled_spans_df, 1)\n",
    "common_bigrams = most_common_ngrams(labelled_spans_df, 2)\n",
    "common_trigrams = most_common_ngrams(labelled_spans_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in common_words:\n",
    "    if category != 'benefit':\n",
    "        print(f\"Category: {category}\")\n",
    "        print(\"Most common words:\", common_words[category])\n",
    "        print(\"Most common bigrams:\", common_bigrams[category])\n",
    "        print(\"Most common trigrams:\", common_trigrams[category])\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_prinz_green_jobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
