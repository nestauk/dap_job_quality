{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harder dimensions of job quality: \"job design and nature of work\", \"social support and cohesion\", \"voice and representation\"\n",
    "\n",
    "This notebook is in 2 parts.\n",
    "\n",
    "Part 1 explores data that was labelled manually in Prodigy and pulls out common single words, bigrams and trigrams.\n",
    "\n",
    "Part 2 does some very basic keyword search to see how prevalent these N-grams are in a sample of 100,000 job ads from OJO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from dap_job_quality import PROJECT_DIR, BUCKET_NAME, logger\n",
    "from dap_job_quality.getters.ojo_getters import get_ojo_sample\n",
    "from dap_job_quality.getters.data_getters import load_s3_jsonl\n",
    "import dap_job_quality.utils.prodigy_data_utils as pdu\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting the data\n",
    "# def load_jsonl(file_path):\n",
    "#     data = []\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "#         for line_number, line in enumerate(file, 1):\n",
    "#             line = line.strip()  # Remove leading/trailing whitespace\n",
    "#             if not line:\n",
    "#                 # Skip empty lines\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 data.append(srsly.json_loads(line))\n",
    "#             except ValueError as e:  # srsly raises ValueError for JSON errors\n",
    "#                 print(f\"Error parsing JSON on line {line_number}: {e}\")\n",
    "#                 # Optionally, continue to next line or handle error differently\n",
    "#     return data\n",
    "\n",
    "# def download_jsonl_from_s3(bucket_name, s3_file_name, local_file):\n",
    "#     \"\"\"\n",
    "#     Download a file from an S3 bucket\n",
    "\n",
    "#     :param bucket_name: Bucket to download from\n",
    "#     :param s3_file_name: S3 object name\n",
    "#     :param local_file: File path to store the downloaded file\n",
    "#     \"\"\"\n",
    "#     # Create an S3 client\n",
    "#     s3 = boto3.client(\"s3\")\n",
    "\n",
    "#     output_file = None\n",
    "\n",
    "#     # Make sure that the directory where you want to store the file exists\n",
    "#     Path(local_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     try:\n",
    "#         # Download the file\n",
    "#         s3.download_file(bucket_name, s3_file_name, local_file)\n",
    "#         logging.info(\n",
    "#             f\"File {s3_file_name} downloaded from {bucket_name} to {local_file}\"\n",
    "#         )\n",
    "\n",
    "#         output_file = load_jsonl(local_file)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"The file {s3_file_name} was not found in {bucket_name}\")\n",
    "#     except NoCredentialsError:\n",
    "#         print(\"Credentials not available\")\n",
    "\n",
    "#     return output_file\n",
    "    \n",
    "\n",
    "# Other helper functions\n",
    "def tokenize(text, n=2):\n",
    "    \"\"\"Tokenize text into n-grams\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "def most_common_ngrams(df, n=2, label_col='label', text_col='labelled_span', n_most_common=10):\n",
    "    \"\"\"Find the most common n-grams within a category\n",
    "    \"\"\"\n",
    "    category_ngrams = {}\n",
    "    for category in df[label_col].unique():\n",
    "        ngrams_list = []\n",
    "        for text in df[df[label_col] == category][text_col]:\n",
    "            n_grams = tokenize(text, n)\n",
    "            ngrams_list.extend(n_grams)\n",
    "        category_ngrams[category] = Counter(ngrams_list).most_common(n_most_common)\n",
    "    return category_ngrams\n",
    "\n",
    "def find_phrase_and_sentence(text, phrases):\n",
    "    \"\"\"Find a phrase in a text and return the whole sentence containing the phrase\n",
    "    \"\"\"\n",
    "    for phrase in phrases:\n",
    "        if phrase in text.lower():  # Check if the phrase is in the text\n",
    "            # Find the whole sentence containing the phrase\n",
    "            sentence = re.search(r'([^.]*?'+re.escape(phrase)+r'[^.]*\\.)', text, re.IGNORECASE)\n",
    "            if sentence:\n",
    "                return True, sentence.group()\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: labelled data\n",
    "\n",
    "Load a sample of ~60 job ads labelled for \"voice and representation\", \"social support and cohesion\" and \"job design and nature of work\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file1 = PROJECT_DIR / 'dap_job_quality/pipeline/prodigy/labelled_data/20240119_ads_labelled_rosie_downloaded.jsonl'\n",
    "local_file2 = PROJECT_DIR / 'dap_job_quality/pipeline/prodigy/labelled_data/20240123_ads_labelled_rosie_downloaded.jsonl'\n",
    "\n",
    "_ = load_s3_jsonl(BUCKET_NAME, 'job_quality/prodigy/labelled_data/20240119_ads_labelled_rosie.jsonl', local_file1)\n",
    "_ = load_s3_jsonl(BUCKET_NAME, 'job_quality/prodigy/labelled_data/20240123_ads_labelled_rosie.jsonl', local_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "\n",
    "for file in [local_file1, local_file2]:\n",
    "    records = pdu.read_accepted_lines(file)\n",
    "    for record in records:\n",
    "        all_records.append(record)\n",
    "        \n",
    "all_records_deduplicated = []\n",
    "seen_job_ids = set()\n",
    "\n",
    "for item in all_records:\n",
    "    job_id = item['meta']['job_id']\n",
    "    if job_id not in seen_job_ids:\n",
    "        seen_job_ids.add(job_id)\n",
    "        all_records_deduplicated.append(item)\n",
    "        \n",
    "training_data = pdu.get_spans_and_sentences(all_records_deduplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "for job_id, entries in training_data.items():\n",
    "    for entry in entries:\n",
    "        flat_data.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"labelled_span\": entry[\"span\"],\n",
    "            \"full_sentence\": entry[\"sent\"],\n",
    "            \"label\": entry[\"label\"],\n",
    "            \"text\": entry[\"text\"]\n",
    "        })\n",
    "\n",
    "labelled_spans_df = pd.DataFrame(flat_data)\n",
    "\n",
    "labelled_spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_spans_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelled_spans_df['job_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of labels in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for the distribution of labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "labelled_spans_df[(labelled_spans_df['label'] != 'none') & (labelled_spans_df['label'] != 'benefit')]['label'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the sentences that occur under each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What comes under `other_benefits`?\n",
    "other_benefits = labelled_spans_df[labelled_spans_df['label'] == 'other_benefits']\n",
    "other_benefits[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about `6_voice_representation`?\n",
    "voice_representation = labelled_spans_df[labelled_spans_df['label'] == '6_voice_representation']\n",
    "voice_representation[['labelled_span']]\n",
    "# could have just regexed \"equal opportunities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_support = labelled_spans_df[labelled_spans_df['label'] == '5_social_support_cohesion']\n",
    "social_support[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for each label\n",
    "label_categories = labelled_spans_df['label'].unique()\n",
    "label_categories = label_categories[(label_categories != 'benefit') & (label_categories != 'other_benefits') & (label_categories != 'none')]\n",
    "\n",
    "# Create a subplot for each wordcloud in a 2x2 configuration\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, label in enumerate(label_categories):\n",
    "    if i < 4:  # Ensure we don't go out of bounds\n",
    "        ax = axes[i]\n",
    "        text = ' '.join(labelled_spans_df[labelled_spans_df['label'] == label]['labelled_span'].tolist())\n",
    "        wordcloud = WordCloud(width=800, height=800, \n",
    "                              background_color ='white',\n",
    "                              min_font_size = 10).generate(text)\n",
    "        \n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(label)\n",
    "\n",
    "# This will ensure that any extra subplots are not visible\n",
    "for j in range(i+1, 4):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(PROJECT_DIR / 'outputs/figures/wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = most_common_ngrams(labelled_spans_df, 1)\n",
    "common_bigrams = most_common_ngrams(labelled_spans_df, 2)\n",
    "common_trigrams = most_common_ngrams(labelled_spans_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in common_words:\n",
    "    if category != 'benefit':\n",
    "        print(f\"Category: {category}\")\n",
    "        print(\"Most common words:\", common_words[category])\n",
    "        print(\"Most common bigrams:\", common_bigrams[category])\n",
    "        print(\"Most common trigrams:\", common_trigrams[category])\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: relate insights from labelled data to OJO sample\n",
    "\n",
    "Load in a sample of 100,000 OJO ads and perform basic keyword searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data = get_ojo_sample()\n",
    "unlabelled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword search\n",
    "\n",
    "We make a dict of search terms where the key is the CIPD dimension or subdimension and the values are the exact terms to search for. These are pruned versions of the bigrams/trigrams above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = {\"job design and nature of work\": ['career progression', 'career prospects', 'career development', 'progression opportunities', 'ongoing training', \"opportunities for progression\", 'learning and development'],\n",
    "           \"social support and cohesion\": [\"relaxed environment\", \"friendly working environment\", \"ongoing support\", \"great culture\"],\n",
    "           # reward/recognition seems a little trickier, so I decided to see how many ads contained these phrases specifically\n",
    "           \"reward\": [\"making a difference\", \"make a difference\", \"sense of purpose\"]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_results = unlabelled_data[unlabelled_data['description'].str.contains('|'.join(phrases[\"reward\"]))]\n",
    "purpose_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = unlabelled_data[unlabelled_data['description'].str.contains('|'.join(phrases[\"job design and nature of work\"]))]\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_results = unlabelled_data[unlabelled_data['description'].str.contains('|'.join(phrases[\"social support and cohesion\"]))]\n",
    "len(social_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(search_results)/len(unlabelled_data)} of the job descriptions contain at least one of the career progression phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for (a) job design and nature of work phrases; (b) more specific purpose-related phrases\n",
    "unlabelled_data[[\"jdnw_phrase\", \"jdnw_sentence\"]] = unlabelled_data.apply(lambda row: find_phrase_and_sentence(row[\"description\"], phrases[\"job design and nature of work\"]), axis=1, result_type='expand')\n",
    "unlabelled_data[[\"purpose_phrase\", \"purpose_sentence\"]] = unlabelled_data.apply(lambda row: find_phrase_and_sentence(row[\"description\"], phrases[\"reward\"]), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of exact mentions of 'learning and development': {sum('learning and development' in desc.lower() for desc in unlabelled_data['description'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy collocations\n",
    "\n",
    "... a possible next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_prinz_green_jobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
