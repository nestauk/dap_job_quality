{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harder dimensions of job quality: \"job design and nature of work\", \"social support and cohesion\", \"voice and representation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "import srsly\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from dap_job_quality import PROJECT_DIR, BUCKET_NAME, logging\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting the data\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                data.append(srsly.json_loads(line))\n",
    "            except ValueError as e:  # srsly raises ValueError for JSON errors\n",
    "                print(f\"Error parsing JSON on line {line_number}: {e}\")\n",
    "                # Optionally, continue to next line or handle error differently\n",
    "    return data\n",
    "\n",
    "def download_file_from_s3(bucket_name, s3_file_name, local_file):\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket\n",
    "\n",
    "    :param bucket_name: Bucket to download from\n",
    "    :param s3_file_name: S3 object name\n",
    "    :param local_file: File path to store the downloaded file\n",
    "    \"\"\"\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    output_file = None\n",
    "\n",
    "    # Make sure that the directory where you want to store the file exists\n",
    "    Path(local_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Download the file\n",
    "        s3.download_file(bucket_name, s3_file_name, local_file)\n",
    "        logging.info(\n",
    "            f\"File {s3_file_name} downloaded from {bucket_name} to {local_file}\"\n",
    "        )\n",
    "\n",
    "        output_file = load_jsonl(local_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {s3_file_name} was not found in {bucket_name}\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "\n",
    "    return output_file\n",
    "    \n",
    "\n",
    "# Define helper functions\n",
    "def tokenize(text, n=2):\n",
    "    \"\"\"Tokenize text into n-grams\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "def most_common_ngrams(df, n=2, label_col='label', text_col='labelled_span', n_most_common=10):\n",
    "    \"\"\"Find the most common n-grams within a category\n",
    "    \"\"\"\n",
    "    category_ngrams = {}\n",
    "    for category in df[label_col].unique():\n",
    "        ngrams_list = []\n",
    "        for text in df[df[label_col] == category][text_col]:\n",
    "            n_grams = tokenize(text, n)\n",
    "            ngrams_list.extend(n_grams)\n",
    "        category_ngrams[category] = Counter(ngrams_list).most_common(n_most_common)\n",
    "    return category_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample of ~60 job ads labelled for \"voice and representation\", \"social support and cohesion\" and \"job design and nature of work\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = PROJECT_DIR / 'dap_job_quality/pipeline/prodigy/labelled_data/20240119_ads_labelled_rosie.jsonl'\n",
    "local_file = 'dap_job_quality/pipeline/prodigy/labelled_data/20240119_ads_labelled_rosie_downloaded.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data as a dataframe first of all, to see what fields it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = download_file_from_s3(BUCKET_NAME, 'job_quality/prodigy/labelled_data/20240119_ads_labelled_rosie.jsonl', local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL data\n",
    "data = []\n",
    "for line in srsly.read_jsonl(local_file):\n",
    "    data.append(line)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in and find the labelled spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for line in srsly.read_jsonl(local_file):\n",
    "    if line[\"answer\"] == \"accept\":\n",
    "        records.append(line)\n",
    "\n",
    "training_data = {}\n",
    "\n",
    "for record in records:\n",
    "    # convert each text to a spacy document\n",
    "    doc = nlp(record['text'])\n",
    "    all_sents = list(doc.sents)\n",
    "    # get the labelled spans within each document\n",
    "    spans = record[\"spans\"]\n",
    "    spans_parsed = []\n",
    "    # map the span back to the text it corresponds to\n",
    "    for span in spans:\n",
    "        span_data = {}\n",
    "        span_data[\"sent\"] = Span(\n",
    "                        doc,\n",
    "                        span[\"token_start\"],\n",
    "                        span[\"token_end\"] + 1,\n",
    "                        span[\"label\"],\n",
    "                    ).text\n",
    "        span_data[\"label\"] = span[\"label\"]\n",
    "        span_data[\"text\"] = record['text']\n",
    "        spans_parsed.append(span_data)\n",
    "    training_data[record[\"meta\"][\"job_id\"]] = spans_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing the parsed, labelled spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "for job_id, entries in training_data.items():\n",
    "    for entry in entries:\n",
    "        flat_data.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"labelled_span\": entry[\"sent\"],\n",
    "            \"label\": entry[\"label\"],\n",
    "            \"text\": entry[\"text\"]\n",
    "        })\n",
    "\n",
    "labelled_spans_df = pd.DataFrame(flat_data)\n",
    "\n",
    "labelled_spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelled_spans_df['job_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_spans_df.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of labels in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for the distribution of labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "labelled_spans_df['label'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the sentences that occur under each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What comes under `other_benefits`?\n",
    "other_benefits = labelled_spans_df[labelled_spans_df['label'] == 'other_benefits']\n",
    "other_benefits[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about `6_voice_representation`?\n",
    "voice_representation = labelled_spans_df[labelled_spans_df['label'] == '6_voice_representation']\n",
    "voice_representation[['labelled_span']]\n",
    "# could have just regexed \"equal opportunities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_support = labelled_spans_df[labelled_spans_df['label'] == '5_social_support_cohesion']\n",
    "social_support[['labelled_span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for each label\n",
    "label_categories = labelled_spans_df['label'].unique()\n",
    "label_categories = label_categories[(label_categories != 'benefit') & (label_categories != 'other_benefits')]\n",
    "\n",
    "# Create a subplot for each wordcloud\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))\n",
    "#fig.suptitle('Wordclouds for Each Label', fontsize=16)\n",
    "\n",
    "for i, label in enumerate(label_categories):\n",
    "    ax = axes[i]\n",
    "    text = ' '.join(labelled_spans_df[labelled_spans_df['label'] == label]['labelled_span'].tolist())\n",
    "    wordcloud = WordCloud(width=800, height=800, \n",
    "                          background_color ='white',\n",
    "                          min_font_size = 10).generate(text)\n",
    "    \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig('wordlcouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = most_common_ngrams(labelled_spans_df, 1)\n",
    "common_bigrams = most_common_ngrams(labelled_spans_df, 2)\n",
    "common_trigrams = most_common_ngrams(labelled_spans_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in common_words:\n",
    "    if category != 'benefit':\n",
    "        print(f\"Category: {category}\")\n",
    "        print(\"Most common words:\", common_words[category])\n",
    "        print(\"Most common bigrams:\", common_bigrams[category])\n",
    "        print(\"Most common trigrams:\", common_trigrams[category])\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_prinz_green_jobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
